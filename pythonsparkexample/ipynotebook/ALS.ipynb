{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597017298845",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"ALS\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global Path    \n",
    "if sc.master[0:5]==\"local\" :\n",
    "   Path=\"/mnt/data1/workspace/data_analysis_mining/Python_Spark/pythonsparkexample/PythonProject\"\n",
    "else:   \n",
    "   Path=\"hdfs://master:9000/user/hduser/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mSparseMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnumRows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnumCols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcolPtrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrowIndices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0misTransposed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m      Sparse Matrix stored in CSC format.\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/ml/linalg/__init__.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "SparseMatrix? \n",
    "# 列压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nCompressed Sparse Column matrix\n\nThis can be instantiated in several ways:\n\n    csc_matrix(D)\n        with a dense matrix or rank-2 ndarray D\n\n    csc_matrix(S)\n        with another sparse matrix S (equivalent to S.tocsc())\n\n    csc_matrix((M, N), [dtype])\n        to construct an empty matrix with shape (M, N)\n        dtype is optional, defaulting to dtype='d'.\n\n    csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n        where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n        relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n\n    csc_matrix((data, indices, indptr), [shape=(M, N)])\n        is the standard CSC representation where the row indices for\n        column i are stored in ``indices[indptr[i]:indptr[i+1]]``\n        and their corresponding values are stored in\n        ``data[indptr[i]:indptr[i+1]]``.  If the shape parameter is\n        not supplied, the matrix dimensions are inferred from\n        the index arrays.\n\nAttributes\n----------\ndtype : dtype\n    Data type of the matrix\nshape : 2-tuple\n    Shape of the matrix\nndim : int\n    Number of dimensions (this is always 2)\nnnz\n    Number of stored values, including explicit zeros\ndata\n    Data array of the matrix\nindices\n    CSC format index array\nindptr\n    CSC format index pointer array\nhas_sorted_indices\n    Whether indices are sorted\n\nNotes\n-----\n\nSparse matrices can be used in arithmetic operations: they support\naddition, subtraction, multiplication, division, and matrix power.\n\nAdvantages of the CSC format\n    - efficient arithmetic operations CSC + CSC, CSC * CSC, etc.\n    - efficient column slicing\n    - fast matrix vector products (CSR, BSR may be faster)\n\nDisadvantages of the CSC format\n  - slow row slicing operations (consider CSR)\n  - changes to the sparsity structure are expensive (consider LIL or DOK)\n\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from scipy.sparse import csc_matrix\n>>> csc_matrix((3, 4), dtype=np.int8).toarray()\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]], dtype=int8)\n\n>>> row = np.array([0, 2, 2, 0, 1, 2])\n>>> col = np.array([0, 0, 1, 2, 2, 2])\n>>> data = np.array([1, 2, 3, 4, 5, 6])\n>>> csc_matrix((data, (row, col)), shape=(3, 3)).toarray()\narray([[1, 0, 4],\n       [0, 0, 5],\n       [2, 3, 6]])\n\n>>> indptr = np.array([0, 2, 3, 6])\n>>> indices = np.array([0, 2, 2, 0, 1, 2])\n>>> data = np.array([1, 2, 3, 4, 5, 6])\n>>> csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()\narray([[1, 0, 4],\n       [0, 0, 5],\n       [2, 3, 6]])\n\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/scipy/sparse/csc.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "csc_matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵中第j列非零元素的行号为indices[indptr[i]:indptr[i+1]]，相应的值为data[indptr[i]:indptr[i+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 0, 4],\n       [0, 0, 5],\n       [2, 3, 6]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "matrix = csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 2, 2, 0, 1, 2], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "matrix.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 2, 3, 6], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "matrix.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 2, 3, 4, 5, 6], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "matrix.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_csc =  SparseMatrix(\n",
    "    3, \n",
    "    3, \n",
    "    colPtrs = matrix.indptr,\n",
    "    rowIndices=matrix.indices,\n",
    "    values=matrix.data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 0., 4.],\n       [0., 0., 5.],\n       [2., 3., 6.]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ss_csc.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseMatrix(3, 3, [1.0, 0.0, 2.0, 0.0, 0.0, 3.0, 4.0, 5.0, 6.0], False)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "ss_csc.toDense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    os.path.join(Path, 'data/u.data'),sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---+---------+\n|_c0|_c1|_c2|      _c3|\n+---+---+---+---------+\n|196|242|  3|881250949|\n|186|302|  3|891717742|\n| 22|377|  1|878887116|\n+---+---+---+---------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+----+------+\n|user|item|rating|\n+----+----+------+\n| 196| 242|     3|\n| 186| 302|     3|\n|  22| 377|     1|\n+----+----+------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "data = data.withColumnRenamed('_c0', 'user').withColumnRenamed('_c1', 'item').withColumnRenamed('_c2', 'rating').select('user', 'item', 'rating')\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'3'"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "data.head().rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3.0"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "float('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_user_data = sc.textFile(os.path.join(Path, 'data/u.data'))\n",
    "ratings_rdd = raw_user_data \\\n",
    "        .map(lambda line: line.split('\\t')[:3]) \\\n",
    "        .map(\n",
    "            lambda x: (\n",
    "                    int(x[0]), int(x[1]), float(x[2])\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(fields=[\n",
    "    StructField('user', IntegerType(), True),\n",
    "    StructField('item', IntegerType(), True),\n",
    "    StructField('rating', FloatType(), True),\n",
    "])\n",
    "data = spark.createDataFrame(ratings_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- user: integer (nullable = true)\n |-- item: integer (nullable = true)\n |-- rating: float (nullable = true)\n\n"
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+----+------+\n|user|item|rating|\n+----+----+------+\n| 196| 242|   3.0|\n| 186| 302|   3.0|\n|  22| 377|   1.0|\n+----+----+------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(rank=10, maxIter=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(rank=10, maxIter=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "69944"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rank,迭代次数, 正则化, 置信参数\n",
    "grid_search = ParamGridBuilder()\\\n",
    "            .addGrid(als.rank, [10, 20, 30]) \\\n",
    "            .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "            .addGrid(als.regParam, [0.1, 0.01, 1]) \\\n",
    "            .addGrid(als.alpha, [0.5, 1., 10.]) \\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \n.. note:: Experimental\n\nEvaluator for Regression, which expects two input\ncolumns: prediction and label.\n\n>>> scoreAndLabels = [(-28.98343821, -27.0), (20.21491975, 21.5),\n...   (-25.98418959, -22.0), (30.69731842, 33.0), (74.69283752, 71.0)]\n>>> dataset = spark.createDataFrame(scoreAndLabels, [\"raw\", \"label\"])\n...\n>>> evaluator = RegressionEvaluator(predictionCol=\"raw\")\n>>> evaluator.evaluate(dataset)\n2.842...\n>>> evaluator.evaluate(dataset, {evaluator.metricName: \"r2\"})\n0.993...\n>>> evaluator.evaluate(dataset, {evaluator.metricName: \"mae\"})\n2.649...\n>>> re_path = temp_path + \"/re\"\n>>> evaluator.save(re_path)\n>>> evaluator2 = RegressionEvaluator.load(re_path)\n>>> str(evaluator2.getPredictionCol())\n'raw'\n\n.. versionadded:: 1.4.0\n\u001b[0;31mInit docstring:\u001b[0m __init__(self, predictionCol=\"prediction\", labelCol=\"label\",                  metricName=\"rmse\")\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/ml/evaluation.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "RegressionEvaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    predictionCol='prediction',\n",
    "    labelCol='rating',\n",
    "    metricName='mae'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=grid_search,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsmodel = tvs.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_param(model):\n",
    "    result = [\n",
    "        (\n",
    "            [\n",
    "                {key.name: param_value} for key, param_value in zip(\n",
    "                    param.keys(), param.values())\n",
    "            ], metric\n",
    "        ) for param, metric in zip(\n",
    "            model.getEstimatorParamMaps(),\n",
    "            model.validationMetrics)  # validationMetrics  avgMetrics\n",
    "    ]\n",
    "    # ([{'maxIter': 50}, {'regParam': 0.01}], 0.7385557487596289)\n",
    "    best_param = sorted(result, key=lambda e: e[1], reverse=True)[0]\n",
    "    return best_param[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = get_best_param(tvsmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'rank': 10}, {'maxIter': 5}, {'regParam': 0.1}, {'alpha': 0.5}]\n"
    }
   ],
   "source": [
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tvsmodel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = best_model.recommendForUserSubset(test_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+-------------------+\n|user|    recommendations|\n+----+-------------------+\n| 148|[[1159, 5.0788465]]|\n| 463|   [[61, 4.082278]]|\n| 471| [[1427, 5.779977]]|\n| 496|[[1467, 4.2834516]]|\n| 833| [[865, 4.5775824]]|\n| 243|[[1398, 4.5864806]]|\n| 392| [[1643, 5.123408]]|\n| 540| [[169, 4.7467027]]|\n| 623|[[1643, 5.2861347]]|\n| 737|[[1193, 5.0169325]]|\n+----+-------------------+\nonly showing top 10 rows\n\n"
    }
   ],
   "source": [
    "rec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}