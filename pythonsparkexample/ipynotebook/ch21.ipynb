{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意事项:\n",
    "#当运行本Notebook的程序后，如果要关闭Notebook，请选择菜单: File > Close and Halt 才能确实停止当前正在运行的程序，并且释放资源\n",
    "#如果没有使用以上方法，只关闭此分页，程序仍在运行，未释放资源，当您打开并运行其他的Notebook，可能会发生错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21\tSpark ML Pipeline多元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"ML multi\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'local[4]'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.1\t数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global Path    \n",
    "if sc.master[0:5]==\"local\" :\n",
    "   Path=\"/mnt/data1/workspace/data_analysis_mining/Python+Spark2.0+Hadoop机器学习与大数据实战/pythonsparkexample/PythonProject/\"\n",
    "else:   \n",
    "   Path=\"hdfs://master:9000/user/hduser/\"\n",
    "#如果要在cluster模式运行(hadoop yarn 或Spark Stand alone)，请按照书上的说明，先把文件上传到HDFS目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "581012"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "rawData = spark.sparkContext.textFile(Path+\"data/covtype.data\")\n",
    "lines = rawData.map(lambda x: x.split(\",\"))\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "55"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "fieldnum = len(lines.first()) \n",
    "fieldnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['2596', '51', '3', '258', '0', '510', '221', '232', '148', '6279']"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "lines.first()[:10]  # 全是数字字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  StringType,StructField,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"f\"+str(i), StringType(), True) \n",
    "               for i in range(fieldnum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_df1 = spark.read.csv(Path+\"data/covtype.data\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39', '_c40', '_c41', '_c42', '_c43', '_c44', '_c45', '_c46', '_c47', '_c48', '_c49', '_c50', '_c51', '_c52', '_c53', '_c54']\n"
    }
   ],
   "source": [
    "print(covtype_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n |-- _c12: string (nullable = true)\n |-- _c13: string (nullable = true)\n |-- _c14: string (nullable = true)\n |-- _c15: string (nullable = true)\n |-- _c16: string (nullable = true)\n |-- _c17: string (nullable = true)\n |-- _c18: string (nullable = true)\n |-- _c19: string (nullable = true)\n |-- _c20: string (nullable = true)\n |-- _c21: string (nullable = true)\n |-- _c22: string (nullable = true)\n |-- _c23: string (nullable = true)\n |-- _c24: string (nullable = true)\n |-- _c25: string (nullable = true)\n |-- _c26: string (nullable = true)\n |-- _c27: string (nullable = true)\n |-- _c28: string (nullable = true)\n |-- _c29: string (nullable = true)\n |-- _c30: string (nullable = true)\n |-- _c31: string (nullable = true)\n |-- _c32: string (nullable = true)\n |-- _c33: string (nullable = true)\n |-- _c34: string (nullable = true)\n |-- _c35: string (nullable = true)\n |-- _c36: string (nullable = true)\n |-- _c37: string (nullable = true)\n |-- _c38: string (nullable = true)\n |-- _c39: string (nullable = true)\n |-- _c40: string (nullable = true)\n |-- _c41: string (nullable = true)\n |-- _c42: string (nullable = true)\n |-- _c43: string (nullable = true)\n |-- _c44: string (nullable = true)\n |-- _c45: string (nullable = true)\n |-- _c46: string (nullable = true)\n |-- _c47: string (nullable = true)\n |-- _c48: string (nullable = true)\n |-- _c49: string (nullable = true)\n |-- _c50: string (nullable = true)\n |-- _c51: string (nullable = true)\n |-- _c52: string (nullable = true)\n |-- _c53: string (nullable = true)\n |-- _c54: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "covtype_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covtype_df = spark.createDataFrame(lines, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  StringType\n",
    "fields = [StructField(\"f\"+str(i), StringType(), True) for i in range(fieldnum )]\n",
    "schema = StructType(fields)\n",
    "covtype_df = spark.createDataFrame(lines, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54']\n"
    }
   ],
   "source": [
    "print(covtype_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- f0: string (nullable = true)\n |-- f1: string (nullable = true)\n |-- f2: string (nullable = true)\n |-- f3: string (nullable = true)\n |-- f4: string (nullable = true)\n |-- f5: string (nullable = true)\n |-- f6: string (nullable = true)\n |-- f7: string (nullable = true)\n |-- f8: string (nullable = true)\n |-- f9: string (nullable = true)\n |-- f10: string (nullable = true)\n |-- f11: string (nullable = true)\n |-- f12: string (nullable = true)\n |-- f13: string (nullable = true)\n |-- f14: string (nullable = true)\n |-- f15: string (nullable = true)\n |-- f16: string (nullable = true)\n |-- f17: string (nullable = true)\n |-- f18: string (nullable = true)\n |-- f19: string (nullable = true)\n |-- f20: string (nullable = true)\n |-- f21: string (nullable = true)\n |-- f22: string (nullable = true)\n |-- f23: string (nullable = true)\n |-- f24: string (nullable = true)\n |-- f25: string (nullable = true)\n |-- f26: string (nullable = true)\n |-- f27: string (nullable = true)\n |-- f28: string (nullable = true)\n |-- f29: string (nullable = true)\n |-- f30: string (nullable = true)\n |-- f31: string (nullable = true)\n |-- f32: string (nullable = true)\n |-- f33: string (nullable = true)\n |-- f34: string (nullable = true)\n |-- f35: string (nullable = true)\n |-- f36: string (nullable = true)\n |-- f37: string (nullable = true)\n |-- f38: string (nullable = true)\n |-- f39: string (nullable = true)\n |-- f40: string (nullable = true)\n |-- f41: string (nullable = true)\n |-- f42: string (nullable = true)\n |-- f43: string (nullable = true)\n |-- f44: string (nullable = true)\n |-- f45: string (nullable = true)\n |-- f46: string (nullable = true)\n |-- f47: string (nullable = true)\n |-- f48: string (nullable = true)\n |-- f49: string (nullable = true)\n |-- f50: string (nullable = true)\n |-- f51: string (nullable = true)\n |-- f52: string (nullable = true)\n |-- f53: string (nullable = true)\n |-- f54: string (nullable = true)\n\nNone\n"
    }
   ],
   "source": [
    "print(covtype_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53']\n"
    }
   ],
   "source": [
    "# 特征字段\n",
    "featuresCols=covtype_df.columns[:54]\n",
    "print(featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- f0: double (nullable = true)\n |-- f1: double (nullable = true)\n |-- f2: double (nullable = true)\n |-- f3: double (nullable = true)\n |-- f4: double (nullable = true)\n |-- f5: double (nullable = true)\n |-- f6: double (nullable = true)\n |-- f7: double (nullable = true)\n |-- f8: double (nullable = true)\n |-- f9: double (nullable = true)\n |-- f10: double (nullable = true)\n |-- f11: double (nullable = true)\n |-- f12: double (nullable = true)\n |-- f13: double (nullable = true)\n |-- f14: double (nullable = true)\n |-- f15: double (nullable = true)\n |-- f16: double (nullable = true)\n |-- f17: double (nullable = true)\n |-- f18: double (nullable = true)\n |-- f19: double (nullable = true)\n |-- f20: double (nullable = true)\n |-- f21: double (nullable = true)\n |-- f22: double (nullable = true)\n |-- f23: double (nullable = true)\n |-- f24: double (nullable = true)\n |-- f25: double (nullable = true)\n |-- f26: double (nullable = true)\n |-- f27: double (nullable = true)\n |-- f28: double (nullable = true)\n |-- f29: double (nullable = true)\n |-- f30: double (nullable = true)\n |-- f31: double (nullable = true)\n |-- f32: double (nullable = true)\n |-- f33: double (nullable = true)\n |-- f34: double (nullable = true)\n |-- f35: double (nullable = true)\n |-- f36: double (nullable = true)\n |-- f37: double (nullable = true)\n |-- f38: double (nullable = true)\n |-- f39: double (nullable = true)\n |-- f40: double (nullable = true)\n |-- f41: double (nullable = true)\n |-- f42: double (nullable = true)\n |-- f43: double (nullable = true)\n |-- f44: double (nullable = true)\n |-- f45: double (nullable = true)\n |-- f46: double (nullable = true)\n |-- f47: double (nullable = true)\n |-- f48: double (nullable = true)\n |-- f49: double (nullable = true)\n |-- f50: double (nullable = true)\n |-- f51: double (nullable = true)\n |-- f52: double (nullable = true)\n |-- f53: double (nullable = true)\n |-- f54: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "# 1 rdd数据 map 转换类型\n",
    "# 2 dataframe 列类型转换\n",
    "from pyspark.sql.functions import col\n",
    "covtype_df = covtype_df.select([col(column).cast(\"double\").alias(column) for column in covtype_df.columns])\n",
    "covtype_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f54作为label \n",
    "covtype_df = covtype_df.withColumn(\"label\", covtype_df.f54 - 1).drop('f54')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\n|    f0|  f1| f2|   f3| f4|   f5|   f6|   f7|   f8|    f9|f10|f11|f12|f13|f14|f15|f16|f17|f18|f19|f20|f21|f22|f23|f24|f25|f26|f27|f28|f29|f30|f31|f32|f33|f34|f35|f36|f37|f38|f39|f40|f41|f42|f43|f44|f45|f46|f47|f48|f49|f50|f51|f52|f53|label|\n+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\n|2596.0|51.0|3.0|258.0|0.0|510.0|221.0|232.0|148.0|6279.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|  4.0|\n+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 1 row\n\n"
    }
   ],
   "source": [
    "covtype_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Row(f0=2596.0, f1=51.0, f2=3.0, f3=258.0, f4=0.0, f5=510.0, f6=221.0, f7=232.0, f8=148.0, f9=6279.0, f10=1.0, f11=0.0, f12=0.0, f13=0.0, f14=0.0, f15=0.0, f16=0.0, f17=0.0, f18=0.0, f19=0.0, f20=0.0, f21=0.0, f22=0.0, f23=0.0, f24=0.0, f25=0.0, f26=0.0, f27=0.0, f28=0.0, f29=0.0, f30=0.0, f31=0.0, f32=0.0, f33=0.0, f34=0.0, f35=0.0, f36=0.0, f37=0.0, f38=0.0, f39=0.0, f40=0.0, f41=0.0, f42=1.0, f43=0.0, f44=0.0, f45=0.0, f46=0.0, f47=0.0, f48=0.0, f49=0.0, f50=0.0, f51=0.0, f52=0.0, f53=0.0, label=4.0)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "covtype_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[f0: double, f1: double, f2: double, f3: double, f4: double, f5: double, f6: double, f7: double, f8: double, f9: double, f10: double, f11: double, f12: double, f13: double, f14: double, f15: double, f16: double, f17: double, f18: double, f19: double, f20: double, f21: double, f22: double, f23: double, f24: double, f25: double, f26: double, f27: double, f28: double, f29: double, f30: double, f31: double, f32: double, f33: double, f34: double, f35: double, f36: double, f37: double, f38: double, f39: double, f40: double, f41: double, f42: double, f43: double, f44: double, f45: double, f46: double, f47: double, f48: double, f49: double, f50: double, f51: double, f52: double, f53: double, label: double]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "covtype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[f0: double, f1: double, f2: double, f3: double, f4: double, f5: double, f6: double, f7: double, f8: double, f9: double, f10: double, f11: double, f12: double, f13: double, f14: double, f15: double, f16: double, f17: double, f18: double, f19: double, f20: double, f21: double, f22: double, f23: double, f24: double, f25: double, f26: double, f27: double, f28: double, f29: double, f30: double, f31: double, f32: double, f33: double, f34: double, f35: double, f36: double, f37: double, f38: double, f39: double, f40: double, f41: double, f42: double, f43: double, f44: double, f45: double, f46: double, f47: double, f48: double, f49: double, f50: double, f51: double, f52: double, f53: double, label: double]"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_df, test_df = covtype_df.randomSplit([0.7, 0.3])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.2\t建立机器学习pipeline管线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, \n",
    "                                                               outputCol=\"features\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\",featuresCol= 'features'\n",
    "                                             ,maxDepth =5,maxBins=20)\n",
    "dt_pipeline = Pipeline(stages=[vectorAssembler ,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[VectorAssembler_50a8fb5dc2ce, DecisionTreeClassifier_1b0753554563]"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "dt_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.3\t使用pipeline进行数据处理与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipelineModel=dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_1b0753554563) of depth 5 with 49 nodes"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "pipelineModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_1b0753554563) of depth 5 with 49 nodes\n  If (feature 0 <= 3058.5)\n   If (feature 0 <= 2567.5)\n    If (feature 10 <= 0.5)\n     If (feature 0 <= 2393.0)\n      If (feature 3 <= 15.0)\n       Predict: 3.0\n      Else (feature 3 > 15.0)\n       Predict: 2.0\n     Else (feature 0 > 2393.0)\n      Predict: 2.0\n    Else (feature 10 > 0.5)\n     If (feature 22 <= 0.5)\n      Predict: 1.0\n     Else (feature 22 > 0.5)\n      If (feature 5 <= 930.5)\n      \n"
    }
   ],
   "source": [
    "print(pipelineModel.stages[1].toDebugString[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.4\t使用pipelineModel 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'label', 'features', 'rawPrediction', 'probability', 'prediction']\n"
    }
   ],
   "source": [
    "print(predicted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------------+--------------------+--------------------+-----+----------+\n|            features|       rawPrediction|         probability|label|prediction|\n+--------------------+--------------------+--------------------+-----+----------+\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  2.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  5.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  2.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  5.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  2.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  5.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  2.0|       2.0|\n|(54,[0,1,2,5,6,7,...|[0.0,44.0,442.0,7...|[0.0,0.0296695886...|  5.0|       3.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  5.0|       2.0|\n|(54,[0,1,2,3,4,5,...|[0.0,422.0,11597....|[0.0,0.0237706303...|  2.0|       2.0|\n+--------------------+--------------------+--------------------+-----+----------+\nonly showing top 10 rows\n\n"
    }
   ],
   "source": [
    "predicted.select('features', 'rawPrediction', 'probability', 'label','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0297, 0.298, 0.5131, 0.0, 0.1591, 0.0]), prediction=3.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0),\n Row(probability=DenseVector([0.0, 0.0238, 0.6532, 0.0631, 0.0, 0.2598, 0.0]), prediction=2.0)]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "predicted.select(['probability', 'prediction']).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21.5\t评估模型的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                            labelCol=\"label\", predictionCol=\"prediction\", \n",
    "                            metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7009438563272801"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "predictions=pipelineModel.transform(test_df)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#21.6\t使用TrainValidation进行训练评估找出最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(dt.impurity, [ \"gini\",\"entropy\"])\\\n",
    "  .addGrid(dt.maxDepth, [ 10,15,25])\\\n",
    "  .addGrid(dt.maxBins, [30,40,50])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evaluator,\n",
    "                  estimatorParamMaps=paramGrid,trainRatio=0.8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs_pipeline = Pipeline(stages=[vectorAssembler , tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs_pipelineModel =tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_1b0753554563) of depth 25 with 45029 nodes\n  If (feature 0 <= 2716.5)\n   If (feature 0 <= 2513.5)\n    If (feature 0 <= 2437.5)\n     If (feature 23 <= 0.5)\n      If (feature 3 <= 15.0)\n       If (feature 12 <= 0.5)\n        If (feature 5 <= 659.5)\n         If (feature 29 <= 0.5)\n          If (feature 6 <= 209.5)\n           If (feature 14 <= 0.5)\n            If (feature 5 <= 214.0)\n             If (feature 18 <= 0.5)\n              If (feat\n"
    }
   ],
   "source": [
    "bestModel=tvs_pipelineModel.stages[1].bestModel\n",
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "bestModel.getOrDefault('maxDepth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+-----+----+-----+-----+----------+\n",
      "|    海拔|  方位|  斜率| 垂直距离|水平距离|   阴影|label|prediction|\n",
      "+------+----+----+-----+----+-----+-----+----------+\n",
      "|1863.0|37.0|17.0|120.0|18.0| 90.0|  5.0|       5.0|\n",
      "|1866.0|23.0|14.0| 85.0|16.0|108.0|  2.0|       2.0|\n",
      "|1871.0|22.0|22.0| 60.0|12.0| 85.0|  5.0|       5.0|\n",
      "|1871.0|36.0|19.0|134.0|26.0|120.0|  5.0|       5.0|\n",
      "|1874.0|18.0|14.0|  0.0| 0.0| 90.0|  5.0|       2.0|\n",
      "|1877.0|19.0|18.0| 85.0|25.0|108.0|  2.0|       2.0|\n",
      "|1879.0|18.0|14.0|  0.0| 0.0|120.0|  5.0|       2.0|\n",
      "|1880.0|13.0|23.0| 90.0|29.0| 67.0|  5.0|       5.0|\n",
      "|1883.0|27.0|24.0|120.0|24.0|108.0|  5.0|       5.0|\n",
      "|1883.0|29.0|24.0| 60.0|24.0|108.0|  5.0|       5.0|\n",
      "+------+----+----+-----+----+-----+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = tvs_pipelineModel.transform(test_df)\n",
    "result=predictions.withColumnRenamed(\"f0\", \"海拔\") \\\n",
    "                                           .withColumnRenamed(\"f1\", \"方位\") \\\n",
    "                                           .withColumnRenamed(\"f2\", \"斜率\") \\\n",
    "                                           .withColumnRenamed(\"f3\", \"垂直距离\") \\\n",
    "                                           .withColumnRenamed(\"f4\", \"水平距离\") \\\n",
    "                                           .withColumnRenamed(\"f5\", \"阴影\")           \n",
    "result.select(\"海拔\",\"方位\",\"斜率\",\"垂直距离\" , \"水平距离\",\"阴影\",\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283691115086464"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}