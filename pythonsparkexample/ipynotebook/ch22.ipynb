{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意事项:\n",
    "#当运行本Notebook的程序后，如果要关闭Notebook，请选择菜单: File > Close and Halt 才能确实停止当前正在运行的程序，并且释放资源\n",
    "#如果没有使用以上方法，只关闭此分页，程序仍在运行，未释放资源，当您打开并运行其他的Notebook，可能会发生错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22\tSpark ML Pipeline 回归分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.1\t数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(\"ML multi\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'local[4]'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global Path    \n",
    "if sc.master[0:5]==\"local\" :\n",
    "   Path=\"/mnt/data1/workspace/data_analysis_mining/Python+Spark2.0+Hadoop机器学习与大数据实战/pythonsparkexample/PythonProject/\"\n",
    "else:   \n",
    "   Path=\"hdfs://master:9000/user/hduser/\"\n",
    "#如果要在cluster模式运行(hadoop yarn 或Spark Stand alone)，请按照书上的说明，先把文件上传到HDFS目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17379"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "hour_df= spark.read.format('csv') \\\n",
    "                  .option(\"header\", 'true').load(Path+\"data/hour.csv\")\n",
    "hour_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\n"
    }
   ],
   "source": [
    "print(hour_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df = hour_df.drop(\"instant\").drop('dteday').drop('yr').drop('casual').drop('registered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- season: string (nullable = true)\n |-- mnth: string (nullable = true)\n |-- hr: string (nullable = true)\n |-- holiday: string (nullable = true)\n |-- weekday: string (nullable = true)\n |-- workingday: string (nullable = true)\n |-- weathersit: string (nullable = true)\n |-- temp: string (nullable = true)\n |-- atemp: string (nullable = true)\n |-- hum: string (nullable = true)\n |-- windspeed: string (nullable = true)\n |-- cnt: string (nullable = true)\n\nNone\n"
    }
   ],
   "source": [
    "print(hour_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_df= hour_df.select([col(column).cast(\"double\").alias(column) \n",
    "                        for column in hour_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- season: double (nullable = true)\n |-- mnth: double (nullable = true)\n |-- hr: double (nullable = true)\n |-- holiday: double (nullable = true)\n |-- weekday: double (nullable = true)\n |-- workingday: double (nullable = true)\n |-- weathersit: double (nullable = true)\n |-- temp: double (nullable = true)\n |-- atemp: double (nullable = true)\n |-- hum: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- cnt: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "hour_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n|   1.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.81|      0.0|16.0|\n|   1.0| 1.0|1.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|40.0|\n|   1.0| 1.0|2.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|32.0|\n|   1.0| 1.0|3.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|13.0|\n|   1.0| 1.0|4.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0| 1.0|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "hour_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[season: double, mnth: double, hr: double, holiday: double, weekday: double, workingday: double, weathersit: double, temp: double, atemp: double, hum: double, windspeed: double, cnt: double]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train_df, test_df = hour_df.randomSplit([0.7, 0.3])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.2\t建立机器学习pipeline管线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer, VectorIndexer,VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n"
    }
   ],
   "source": [
    "featuresCols = hour_df.columns[:-1]\n",
    "print(featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([-1.0, 0.5]),),\n",
    "                            (Vectors.dense([0.0, 1.0]),), \n",
    "                            (Vectors.dense([0.0, 2.0]),),\n",
    "                            (Vectors.dense([1.0, 4.0]),),\n",
    "                            ],\n",
    "                           [\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+\n|         a|\n+----------+\n|[-1.0,0.5]|\n| [0.0,1.0]|\n| [0.0,2.0]|\n| [1.0,4.0]|\n+----------+\n\n"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(maxCategories=3, inputCol=\"a\", outputCol=\"indexed\")\n",
    "model = indexer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+---------+\n|         a|  indexed|\n+----------+---------+\n|[-1.0,0.5]|[1.0,0.5]|\n| [0.0,1.0]|[0.0,1.0]|\n| [0.0,2.0]|[0.0,2.0]|\n| [1.0,4.0]|[2.0,4.0]|\n+----------+---------+\n\n"
    }
   ],
   "source": [
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"aFeatures\")\n",
    "# 将星期 月份 小时 等会被视为分类字段\n",
    "vectorIndexer = VectorIndexer(inputCol=\"aFeatures\", outputCol=\"features\", maxCategories=24)\n",
    "dt = DecisionTreeRegressor(labelCol=\"cnt\",featuresCol= 'features')\n",
    "dt_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[VectorAssembler_6413f133859f,\n VectorIndexer_a98677683853,\n DecisionTreeRegressor_109fbc85c5f7]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dt_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.3\t使用pipeline进行数据处理与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_pipelineModel = dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_109fbc85c5f7) of depth 5 with 63 nodes"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "dt_pipelineModel.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_109fbc85c5f7) of depth 5 with 63 nodes\n  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n    If (feature 2 in {2.0,3.0,4.0,5.0})\n     If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n      If (feature 2 in {2.0,3.0,4.0})\n       Predict: 6.790719696969697\n      Else (feature 2 not in {2.0,3.0,4.0})\n       Predict: 24.738292011019283\n     Else (feature 4 not in {1.0,2.0,3.0,4.0,5.0})\n      If (fe\n"
    }
   ],
   "source": [
    "print(dt_pipelineModel.stages[2].toDebugString[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.4\t使用pipelineModel 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_df=dt_pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt', 'aFeatures', 'features', 'prediction']\n"
    }
   ],
   "source": [
    "print(predicted_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\n|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|       prediction|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\n|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0|54.08571428571429|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.12|0.1212| 0.5|   0.2836| 5.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.18|0.2424|0.86|      0.0|19.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.24|0.2273|0.65|   0.2239| 7.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.32|0.2879|0.26|   0.4179|10.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0|0.22|0.2424|0.87|   0.1045|14.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0| 0.3|0.2879| 1.0|   0.2836|25.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    4.0|       1.0|       1.0|0.14|0.1364| 0.5|    0.194|16.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       1.0|0.22|0.2576|0.75|   0.0896|25.0|37.77808988764045|\n|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       2.0| 0.2|0.2121|0.75|   0.1343| 9.0|37.77808988764045|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\nonly showing top 10 rows\n\n"
    }
   ],
   "source": [
    "predicted_df.select('season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', \\\n",
    "                     'weathersit', 'temp', 'atemp', 'hum', 'windspeed','cnt','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.5\t评估模型的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol='cnt',\n",
    "                                                        predictionCol='prediction',\n",
    "                                                        metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "98.3475020549714"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "predicted_df=dt_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.6\t使用TrainValidation进行训练评估找出最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(dt.maxDepth, [ 5,10,15,25])\\\n",
    "  .addGrid(dt.maxBins, [25,35,45,50])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evaluator,\n",
    "                  estimatorParamMaps=paramGrid,trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs_pipelineModel =tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_109fbc85c5f7) of depth 10 with 1803 nodes\n  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n    If (feature 2 in {2.0,3.0,4.0,5.0})\n     If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n      If (feature 2 in {2.0,3.0,4.0})\n       If (feature 2 in {3.0,4.0})\n        If (feature 1 in {0.0,1.0,2.0,3.0,11.0})\n         If (feature 7 <= 0.41000000000000003)\n          If (feature 0 in {0.0,1.0})\n    \n"
    }
   ],
   "source": [
    "bestModel=tvs_pipelineModel.stages[2].bestModel\n",
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "81.27945069184695"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "predictions = tvs_pipelineModel.transform(test_df)\n",
    "rmse= evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.7\t使用crossValidation进行训练评估找出最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=dt, evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.13273892536569"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cv_pipelineModel.transform(test_df)\n",
    "rmse= evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.8\t使用随机森林RandomForestClassifier分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "103.14404734402089"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"oFeatures\")\n",
    "vectorIndexer = VectorIndexer(inputCol=\"oFeatures\", outputCol=\"features\", maxCategories=24)\n",
    "rf= RandomForestRegressor(labelCol=\"cnt\",featuresCol= 'features', numTrees=20)\n",
    "rf_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,rf])\n",
    "rf_pipelineModel = rf_pipeline.fit(train_df)\n",
    "predicted_df=rf_pipelineModel.transform(test_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol='cnt', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "70.73193772509443"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(rf.maxDepth, [ 5,10,15])\\\n",
    "  .addGrid(rf.maxBins, [25,35,50])\\\n",
    "  .addGrid(rf.numTrees, [10, 20,30])\\\n",
    "  .build()\n",
    "\n",
    "rftvs = TrainValidationSplit(estimator=rf, evaluator=evaluator,\n",
    "                                 estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
    "\n",
    "rftvs_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer, rftvs])\n",
    "rftvs_pipelineModel =rftvs_pipeline.fit(train_df)\n",
    "rftvspredictions = rftvs_pipelineModel.transform(test_df)\n",
    "rmse= evaluator.evaluate(rftvspredictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 22.8\t使用GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"cnt\",featuresCol= 'features')\n",
    "gbt_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "76.9287732767273"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "gbt_pipelineModel = gbt_pipeline.fit(train_df)\n",
    "predicted_df=gbt_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(gbt.maxDepth, [ 5,10])\\\n",
    "  .addGrid(gbt.maxBins, [25,40])\\\n",
    "  .addGrid(gbt.maxIter, [10, 50])\\\n",
    "  .build()\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, \n",
    "                                  estimatorParamMaps=paramGrid, numFolds=3)\n",
    "cv_pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_109fbc85c5f7) of depth 10 with 1803 nodes\n  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n    If (feature 2 in {2.0,3.0,4.0,5.0})\n     If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n      If (feature 2 in {2.0,3.0,4.0})\n       If (feature 2 in {3.0,4.0})\n        If (feature 1 in {0.0,1.0,2.0,3.0,11.0})\n         If (feature 7 <= 0.41000000000000003)\n          If (feature 0 in {0.0,1.0})\n    \n"
    }
   ],
   "source": [
    "cvm=cv_pipelineModel.stages[2] \n",
    "gbestModel=cvm.bestModel\n",
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0| 74.16023695702242|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.12|0.1212| 0.5|   0.2836| 5.0| 12.88746098409448|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.18|0.2424|0.86|      0.0|19.0| 27.49454199939149|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.24|0.2273|0.65|   0.2239| 7.0|20.909606077888743|\n|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.32|0.2879|0.26|   0.4179|10.0|28.601369929027847|\n|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0|0.22|0.2424|0.87|   0.1045|14.0| 25.08655733253785|\n|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0| 0.3|0.2879| 1.0|   0.2836|25.0| 9.209108647469305|\n|   1.0| 1.0|0.0|    0.0|    4.0|       1.0|       1.0|0.14|0.1364| 0.5|    0.194|16.0| 11.95039375265236|\n|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       1.0|0.22|0.2576|0.75|   0.0896|25.0| 29.84448308873667|\n|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       2.0| 0.2|0.2121|0.75|   0.1343| 9.0|31.555032118176793|\n+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\nonly showing top 10 rows\n\n"
    }
   ],
   "source": [
    "predicted_df=cv_pipelineModel.transform(test_df)\n",
    "predicted_df.select('season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', \\\n",
    "                     'weathersit', 'temp', 'atemp', 'hum', 'windspeed','cnt','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "72.09077346060839"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                labelCol='cnt', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}