{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yarn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x%2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"hadoop-master\"\n",
    "table = \"student\"\n",
    "conf = {\"hbase.zookeeper.quorum\": host, \"hbase.mapreduce.inputtable\": table}\n",
    "# bytes 转为python string\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase_rdd = sc.newAPIHadoopRDD(\"org.apache.hadoop.hbase.mapreduce.TableInputFormat\",\n",
    "                               \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "                               \"org.apache.hadoop.hbase.client.Result\",\n",
    "                               keyConverter=keyConv,\n",
    "                               valueConverter=valueConv,\n",
    "                               conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = hbase_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at mapPartitions at SerDeUtil.scala:244"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hbase_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {\"qualifier\" : \"age\", \"timestamp\" : \"1596267188546\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"19\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1596267181932\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"M\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1596267173284\", \"columnFamily\" : \"info\", \"row\" : \"1\", \"type\" : \"Put\", \"value\" : \"jack\"}\n",
      "2 {\"qualifier\" : \"age\", \"timestamp\" : \"1596267210165\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"18\"}\n",
      "{\"qualifier\" : \"gender\", \"timestamp\" : \"1596267203859\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"F\"}\n",
      "{\"qualifier\" : \"name\", \"timestamp\" : \"1596267196980\", \"columnFamily\" : \"info\", \"row\" : \"2\", \"type\" : \"Put\", \"value\" : \"rose\"}\n"
     ]
    }
   ],
   "source": [
    "output = hbase_rdd.collect()\n",
    "for k, v in output:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'hadoop-master'\n",
    "table = 'student'\n",
    "# python string 转为bytes\n",
    "keyConv = \"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter\"\n",
    "# 字符串列表 写入\n",
    "valueConv = \"org.apache.spark.examples.pythonconverters.StringListToPutConverter\"\n",
    "conf = {\"hbase.zookeeper.quorum\": host,\n",
    "        \"hbase.mapred.outputtable\": table,\n",
    "        \"mapreduce.outputformat.class\": \"org.apache.hadoop.hbase.mapreduce.TableOutputFormat\",\n",
    "        \"mapreduce.job.output.key.class\": \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "        \"mapreduce.job.output.value.class\": \"org.apache.hadoop.io.Writable\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', ['5', 'info', 'name', 'Rongcheng']),\n",
       " ('5', ['5', 'info', 'gender', 'M']),\n",
       " ('5', ['5', 'info', 'age', '26']),\n",
       " ('6', ['6', 'info', 'name', 'Guanhua']),\n",
       " ('6', ['6', 'info', 'gender', 'M']),\n",
       " ('6', ['6', 'info', 'age', '27'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row-key,column-family,column,value\n",
    "raw_data = ['5,info,name,Rongcheng',\n",
    "            '5,info,gender,M',\n",
    "            '5,info,age,26',\n",
    "            '6,info,name,Guanhua',\n",
    "            '6,info,gender,M',\n",
    "            '6,info,age,27']\n",
    "write_rdd = sc.parallelize(raw_data).map(lambda x: (x[0], x.split(',')))\n",
    "write_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write_rdd.saveAsNewAPIHadoopDataset(\n",
    "    conf,\n",
    "    keyConverter=None,\n",
    "    valueConverter=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rdd.saveAsNewAPIHadoopDataset(conf=conf,\n",
    "                                    keyConverter=keyConv, \n",
    "                                    valueConverter=valueConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
