{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "base",
   "display_name": "tf-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换器\n",
    "At the high level, when deriving from the `Transformer` abstract class, each and \n",
    "every new `Transformer` needs to implement a `.transform(...)` method\n",
    "\n",
    "- inputCol, 输入特征列 默认 `\"features\"`\n",
    "- outputCol, 转换输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('feature test').setMaster('local[4]')\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizer\n",
    "\n",
    "Given a threshold, the method takes a continuous variable and \n",
    "transforms it into a binary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+\n|values|\n+------+\n| 0.511|\n|0.6232|\n|0.4323|\n|0.9434|\n|0.3213|\n+------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0.511,), (0.6232,), (0.4323,), (0.9434,), (0.3213,)],\n",
    "                           [\"values\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+--------+\n|values|features|\n+------+--------+\n| 0.511|     1.0|\n|0.6232|     1.0|\n|0.4323|     0.0|\n|0.9434|     1.0|\n|0.3213|     0.0|\n+------+--------+\n\n"
    }
   ],
   "source": [
    "binarizer = ft.Binarizer(threshold=0.5, inputCol=\"values\", outputCol='features')\n",
    "binarizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizer\n",
    "Similar to the Binarizer, this method takes a list of thresholds \n",
    "(the splits parameter) and transforms a continuous variable into a \n",
    "multinomial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+\n|values|\n+------+\n|   0.1|\n|   0.4|\n|   1.2|\n|   1.5|\n|   NaN|\n|   NaN|\n+------+\n\n"
    }
   ],
   "source": [
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, ['values'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+--------+\n|values|features|\n+------+--------+\n|   0.1|     0.0|\n|   0.4|     0.0|\n|   1.2|     1.0|\n|   1.5|     2.0|\n|   NaN|     3.0|\n|   NaN|     3.0|\n+------+--------+\n\n"
    }
   ],
   "source": [
    "bucketizer = ft.Bucketizer(\n",
    "    splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],\n",
    "    inputCol='values', outputCol='features'\n",
    ")\n",
    "# 保持 无效值NaN\n",
    "\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+---+\n|values|  b|\n+------+---+\n|   0.1|0.0|\n|   0.4|0.0|\n|   1.2|1.0|\n|   1.5|2.0|\n|   NaN|3.0|\n|   NaN|3.0|\n+------+---+\n\n"
    }
   ],
   "source": [
    "bucketizer.setParams(outputCol=\"b\").transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "使用卡方检验(Chi-Square) 完成特征选择\n",
    "\n",
    "$\\chi^2 -test$\n",
    "\n",
    "参考  https://blog.csdn.net/sinat_33761963/article/details/54910955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+-----+\n|          features|label|\n+------------------+-----+\n|[0.0,0.0,18.0,1.0]|  1.0|\n|[0.0,1.0,12.0,0.0]|  0.0|\n|[1.0,0.0,15.0,0.1]|  0.0|\n+------------------+-----+\n\n"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame(\n",
    "    [(Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0),\n",
    "     (Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.),\n",
    "     (Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.),],\n",
    "    [\"features\", 'label'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+-----+---------------+\n|          features|label|selectedFeature|\n+------------------+-----+---------------+\n|[0.0,0.0,18.0,1.0]|  1.0|         [18.0]|\n|[0.0,1.0,12.0,0.0]|  0.0|         [12.0]|\n|[1.0,0.0,15.0,0.1]|  0.0|         [15.0]|\n+------------------+-----+---------------+\n\n"
    }
   ],
   "source": [
    "# 选择最优的特征\n",
    "selector = ft.ChiSqSelector(numTopFeatures=1, outputCol='selectedFeature')\n",
    "model = selector.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CountVectorizer\n",
    "处理标记文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+\n|label|            raw|\n+-----+---------------+\n|    0|      [a, b, c]|\n|    1|[a, b, b, c, a]|\n+-----+---------------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "    [\"label\", 'raw']\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+-------------------------+\n|label|raw            |vectors                  |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"
    }
   ],
   "source": [
    "cv = ft.CountVectorizer(minTF=1., minDF=1., \n",
    "                        inputCol='raw', outputCol='vectors')\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['a', 'b', 'c']"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.vocabulary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+-------------------------+\n|label|raw            |vectors                  |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"
    }
   ],
   "source": [
    "fromVocabModel = ft.CountVectorizerModel.from_vocabulary(\n",
    "    ['a', 'b', 'c'],\n",
    "    inputCol='raw',\n",
    "    outputCol='vectors')\n",
    "fromVocabModel.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCT\n",
    "A feature transformer that takes the 1D discrete cosine transform of a real vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElementwiseProduct\n",
    "\n",
    "元素级别的向量乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+\n|       values|\n+-------------+\n|[2.0,1.0,3.0]|\n+-------------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], \n",
    "                           [\"values\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+-------------+\n|       values|        eprod|\n+-------------+-------------+\n|[2.0,1.0,3.0]|[2.0,2.0,9.0]|\n+-------------+-------------+\n\n"
    }
   ],
   "source": [
    "ep = ft.ElementwiseProduct(\n",
    "    scalingVec=Vectors.dense([1.0, 2.0, 3.0]),\n",
    "    inputCol='values',\n",
    "    outputCol='eprod'\n",
    ")\n",
    "ep.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureHasher\n",
    "Feature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick (https://en.wikipedia.org/wiki/Feature_hashing) to map features to indices in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+-----+---------+------+\n|real| bool|stringNum|string|\n+----+-----+---------+------+\n| 2.0| true|        1|   foo|\n| 3.0|false|        2|   bar|\n+----+-----+---------+------+\n\n"
    }
   ],
   "source": [
    "data = [(2.0, True, \"1\", \"foo\"), (3.0, False, \"2\", \"bar\")]\n",
    "cols = [\"real\", \"bool\", \"stringNum\", \"string\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(262144, {174475: 2.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "hasher = ft.FeatureHasher(inputCols=cols, outputCol='features')\n",
    "hasher.transform(df).head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(262144, {171257: 1.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "hasher.setCategoricalCols([\"real\"]).transform(df).head().features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "\n",
    "输入为标记文本的列表, 返回一个带有计数的有预定长度的向量\n",
    "\n",
    "Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a `power of two` as the `numFeatures` parameter; otherwise the features will not be mapped evenly to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+\n|    words|\n+---------+\n|[a, b, c]|\n|[a, c, d]|\n|[a, d, f]|\n+---------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"], ), ([\"a\", \"c\", \"d\"],), (['a', 'd', 'f'],)], [\"words\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+--------------------------+\n|words    |tf_features               |\n+---------+--------------------------+\n|[a, b, c]|(10,[0,1,2],[1.0,1.0,1.0])|\n|[a, c, d]|(10,[0,2,4],[1.0,1.0,1.0])|\n|[a, d, f]|(10,[0,4,8],[1.0,1.0,1.0])|\n+---------+--------------------------+\n\n"
    }
   ],
   "source": [
    "hashingTF = ft.HashingTF(numFeatures=10, inputCol='words', outputCol='tf_features')\n",
    "hashed_data = hashingTF.transform(df)\n",
    "hashed_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(16, {1: 1.0, 2: 1.0, 10: 1.0})"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "params = {hashingTF.numFeatures: 16, hashingTF.outputCol: \"vector\"}\n",
    "hashingTF.transform(df, params).head().vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF\n",
    "逆文档词频, 文档需要提前使用向量表示 如HashingTF或CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------+--------------------------+----------------------------------------------------------+\n|words    |tf_features               |idf_features                                              |\n+---------+--------------------------+----------------------------------------------------------+\n|[a, b, c]|(10,[0,1,2],[1.0,1.0,1.0])|(10,[0,1,2],[0.0,0.6931471805599453,0.28768207245178085]) |\n|[a, c, d]|(10,[0,2,4],[1.0,1.0,1.0])|(10,[0,2,4],[0.0,0.28768207245178085,0.28768207245178085])|\n|[a, d, f]|(10,[0,4,8],[1.0,1.0,1.0])|(10,[0,4,8],[0.0,0.28768207245178085,0.6931471805599453]) |\n+---------+--------------------------+----------------------------------------------------------+\n\n"
    }
   ],
   "source": [
    "idf = ft.IDF(inputCol='tf_features', outputCol='idf_features')\n",
    "model = idf.fit(hashed_data)\n",
    "model.transform(hashed_data).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexToString\n",
    "\n",
    "将字符串索引反转到原始值\n",
    "\n",
    "## StringIndexer \n",
    "一列类别型特征 编码数值化, 索引从0开始\n",
    "\n",
    "优先编码频率较大的标签，所以出现频率最高的标签为0号\n",
    "\n",
    "如果输入的是数值型的，会首先把它转化成字符型，然后再对其进行编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+\n| id|category|\n+---+--------+\n|  0|       a|\n|  1|       b|\n|  2|       c|\n|  3|       a|\n|  4|       a|\n|  5|       c|\n+---+--------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],[\"id\", \"category\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------------+\n| id|category|category_index|\n+---+--------+--------------+\n|  0|       a|           0.0|\n|  1|       b|           2.0|\n|  2|       c|           1.0|\n|  3|       a|           0.0|\n|  4|       a|           0.0|\n|  5|       c|           1.0|\n+---+--------+--------------+\n\n"
    }
   ],
   "source": [
    "indexer = ft.StringIndexer(inputCol='category', outputCol='category_index')\n",
    "model = indexer.fit(df)\n",
    "df_indexed = model.transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------------+---------------+\n| id|category|category_index|origin_category|\n+---+--------+--------------+---------------+\n|  0|       a|           0.0|              a|\n|  1|       b|           2.0|              b|\n|  2|       c|           1.0|              c|\n|  3|       a|           0.0|              a|\n|  4|       a|           0.0|              a|\n|  5|       c|           1.0|              c|\n+---+--------+--------------+---------------+\n\n"
    }
   ],
   "source": [
    "# 从index转换回来\n",
    "toString = ft.IndexToString(inputCol='category_index', outputCol='origin_category')\n",
    "\n",
    "df_string = toString.transform(df_indexed)\n",
    "df_string.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxAbsScaler\n",
    "把数据 调整到\\[-1.0, 1.0\\], 不会移动数据中心\n",
    "\n",
    "## MinMaxScaler\n",
    "把数据 调整到\\[0.0, 1.0\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+\n|          datas|\n+---------------+\n|  [1.0,2.0,3.0]|\n|[-4.0,1.0,-5.0]|\n+---------------+\n\n"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame(\n",
    "    [(Vectors.dense(1.0, 2.0, 3.0),),\n",
    "     (Vectors.dense(-4.0, 1.0, -5.0),)],\n",
    "    ['datas'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+---------------+\n|          datas|       ma_datas|\n+---------------+---------------+\n|  [1.0,2.0,3.0]| [0.25,1.0,0.6]|\n|[-4.0,1.0,-5.0]|[-1.0,0.5,-1.0]|\n+---------------+---------------+\n\n"
    }
   ],
   "source": [
    "maScalar = ft.MaxAbsScaler(inputCol='datas', outputCol='ma_datas')\n",
    "model = maScalar.fit(df)\n",
    "model.transform(df).show() # 每列/每列的最大abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseVector([4.0, 2.0, 5.0])"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "model.maxAbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+-------------+\n|          datas|     mm_datas|\n+---------------+-------------+\n|  [1.0,2.0,3.0]|[1.0,1.0,1.0]|\n|[-4.0,1.0,-5.0]|[0.0,0.0,0.0]|\n+---------------+-------------+\n\n"
    }
   ],
   "source": [
    "mmScalar = ft.MinMaxScaler(min=0.0, max=1.0, inputCol='datas', outputCol='mm_datas')\n",
    "model = mmScalar.fit(df)\n",
    "model.transform(df).show() # 每列/每列的最大abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseVector([1.0, 2.0, 3.0])"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "model.originalMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseVector([-4.0, 1.0, -5.0])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "model.originalMin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram\n",
    "\n",
    "n元词组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+\n|     inputtoken|\n+---------------+\n|[a, b, c, d, e]|\n+---------------+\n\n"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(inputtoken=['a', 'b', 'c', 'd', 'e'])])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Row(inputtoken=['a', 'b', 'c', 'd', 'e'], n-Gram=['a b', 'b c', 'c d', 'd e'])"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "ngram = ft.NGram(n=2, inputCol='inputtoken', outputCol='n-Gram')\n",
    "ngram.transform(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------------+\n|n-Gram               |\n+---------------------+\n|[a b c, b c d, c d e]|\n+---------------------+\n\n"
    }
   ],
   "source": [
    "ngram.setParams(n=3).transform(df).select('n-Gram').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Normalizer\n",
    "根据p范数将数据缩放为单位范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+-------------------+\n|     dense|             sparse|\n+----------+-------------------+\n|[-3.0,4.0]|(4,[1,3],[4.0,3.0])|\n+----------+-------------------+\n\n"
    }
   ],
   "source": [
    "svec = Vectors.sparse(4, {1:4.0, 3:3.0})\n",
    "df = spark.createDataFrame([(Vectors.dense([-3.0, 4.0]), svec)], \n",
    "                           [\"dense\", 'sparse'])\n",
    "df.show()                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Row(dense=DenseVector([-3.0, 4.0]), sparse=SparseVector(4, {1: 4.0, 3: 3.0}), features=DenseVector([-0.6, 0.8]))"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "normalizer = ft.Normalizer(p=2.0, inputCol=\"dense\", outputCol='features')\n",
    "normalizer.transform(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(4, {1: 0.8, 3: 0.6})"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "normalizer.setParams(inputCol=\"sparse\", outputCol='freqs').transform(df).head().freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  StandardScaler\n",
    "\n",
    "0均值, 1方差 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseVector([1.0, 2.0])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "df = spark.createDataFrame([(Vectors.dense([0.0, 1.0]),), (Vectors.dense([2.0, 3.0]),)], [\"a\"])\n",
    "scandardScalar = ft.StandardScaler(\n",
    "    withMean=False, withStd=True,\n",
    "    inputCol='a', outputCol='scaled'\n",
    " )\n",
    "model = scandardScalar.fit(df)\n",
    "model.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseVector([1.4142, 1.4142])"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "model.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(a=DenseVector([0.0, 1.0]), scaled=DenseVector([0.0, 0.7071])),\n Row(a=DenseVector([2.0, 3.0]), scaled=DenseVector([1.4142, 2.1213]))]"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "model.transform(df).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoder \n",
    "将分类列编码为二进制向量列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------------+\n| id|category|category_index|\n+---+--------+--------------+\n|  0|       a|           0.0|\n|  1|       b|           2.0|\n|  2|       c|           1.0|\n|  3|       a|           0.0|\n|  4|       a|           0.0|\n|  5|       c|           1.0|\n+---+--------+--------------+\n\n"
    }
   ],
   "source": [
    "df_indexed.show()  # 如果分类信息是特征的一部分  而不是标签y  使用onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------------+---------------+\n| id|category|category_index|category_onehot|\n+---+--------+--------------+---------------+\n|  0|       a|           0.0|  (3,[0],[1.0])|\n|  1|       b|           2.0|  (3,[2],[1.0])|\n|  2|       c|           1.0|  (3,[1],[1.0])|\n|  3|       a|           0.0|  (3,[0],[1.0])|\n|  4|       a|           0.0|  (3,[0],[1.0])|\n|  5|       c|           1.0|  (3,[1],[1.0])|\n+---+--------+--------------+---------------+\n\n"
    }
   ],
   "source": [
    "encoder = ft.OneHotEncoder(dropLast=False, inputCol='category_index', outputCol='category_onehot')\n",
    "encoder.transform(df_indexed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------------+---------------+\n| id|category|category_index|category_onehot|\n+---+--------+--------------+---------------+\n|  0|       a|           0.0|  (2,[0],[1.0])|\n|  1|       b|           2.0|      (2,[],[])|\n|  2|       c|           1.0|  (2,[1],[1.0])|\n|  3|       a|           0.0|  (2,[0],[1.0])|\n|  4|       a|           0.0|  (2,[0],[1.0])|\n|  5|       c|           1.0|  (2,[1],[1.0])|\n+---+--------+--------------+---------------+\n\n"
    }
   ],
   "source": [
    "encoder.setParams(dropLast=True).transform(df_indexed).show()  # 类别2 全0 其他至少一个为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA \n",
    "降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------------+\n|features             |\n+---------------------+\n|(5,[1,3],[1.0,7.0])  |\n|[2.0,0.0,3.0,4.0,5.0]|\n|[4.0,0.0,0.0,6.0,7.0]|\n+---------------------+\n\n"
    }
   ],
   "source": [
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "    (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "    (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(features=SparseVector(5, {1: 1.0, 3: 7.0}), pca_features=DenseVector([1.6486, -4.0133])),\n Row(features=DenseVector([2.0, 0.0, 3.0, 4.0, 5.0]), pca_features=DenseVector([-4.6451, -1.1168])),\n Row(features=DenseVector([4.0, 0.0, 0.0, 6.0, 7.0]), pca_features=DenseVector([-6.4289, -5.338]))]"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "pca = ft.PCA(k=2, inputCol='features', outputCol='pca_features')\n",
    "model = pca.fit(df)\n",
    "model.transform(df).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DenseMatrix(5, 2, [-0.4486, 0.133, -0.1252, 0.2165, -0.8477, -0.2842, -0.0562, 0.7636, -0.5653, -0.1156], 0)"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "model.pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "分词器 转为 小写, 按空格划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Row(text='a b c', words=['a', 'b', 'c'])"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a b c\",)], [\"text\"])\n",
    "tokenizer = ft.Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Row(text='a b c', words=['a', 'b', 'c'])"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# Temporarily modify a parameter.\n",
    "tokenizer.transform(df, {tokenizer.outputCol: \"words\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler\n",
    "多个数字向量列 合并为1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---+\n|  a|  b|  c|\n+---+---+---+\n|  1|  0|  3|\n|  2|  1|  4|\n+---+---+---+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 0, 3), (2, 1, 4)], [\"a\", \"b\", \"c\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---+-------------+\n|  a|  b|  c|     features|\n+---+---+---+-------------+\n|  1|  0|  3|[1.0,0.0,3.0]|\n|  2|  1|  4|[2.0,1.0,4.0]|\n+---+---+---+-------------+\n\n"
    }
   ],
   "source": [
    "vecAssembler = ft.VectorAssembler(inputCols=['a', 'b', 'c'], outputCol='features')\n",
    "vecAssembler.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorIndexer\n",
    "\n",
    "为类别列label生成索引向量\n",
    "\n",
    "StringIndexer是针对单个类别型特征进行转换，倘若所有特征都已经被组织在一个向量中，又想对其中某些单个分量进行处理时\n",
    "VectorIndexer类来解决向量数据集中的类别性特征转换\n",
    "\n",
    "通过为其提供maxCategories超参数，它可以自动识别哪些特征是类别型的，并且将原始值转换为类别索引。它基于不同特征值的数量来识别哪些特征需要被类别化，那些取值可能性最多不超过maxCategories的特征需要会被认为是类别型的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------+\n|      features|\n+--------------+\n|[-1.0,1.0,1.0]|\n|[-1.0,3.0,1.0]|\n| [0.0,5.0,1.0]|\n+--------------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "     [(Vectors.dense(-1.0, 1.0, 1.0),),\n",
    "      (Vectors.dense(-1.0, 3.0, 1.0),),\n",
    "      (Vectors.dense(0.0, 5.0, 1.0), )],\n",
    "     [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------+-------------+\n|      features|      indexed|\n+--------------+-------------+\n|[-1.0,1.0,1.0]|[1.0,1.0,0.0]|\n|[-1.0,3.0,1.0]|[1.0,3.0,0.0]|\n| [0.0,5.0,1.0]|[0.0,5.0,0.0]|\n+--------------+-------------+\n\n"
    }
   ],
   "source": [
    "indexer = ft.VectorIndexer(maxCategories=2, inputCol='features', outputCol='indexed')\n",
    "model = indexer.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: {0.0: 0, -1.0: 1}, 2: {1.0: 0}}"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "model.categoryMaps  # 共有两个特征被转换，分别是0号和2号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}