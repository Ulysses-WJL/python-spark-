{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "base",
   "display_name": "tf-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换器\n",
    "At the high level, when deriving from the `Transformer` abstract class, each and \n",
    "every new `Transformer` needs to implement a `.transform(...)` method\n",
    "\n",
    "- inputCol, 输入特征列 默认 `\"features\"`\n",
    "- outputCol, 转换输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('feature test').setMaster('local[4]')\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizer\n",
    "\n",
    "Given a threshold, the method takes a continuous variable and \n",
    "transforms it into a binary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+\n|values|\n+------+\n| 0.511|\n|0.6232|\n|0.4323|\n|0.9434|\n|0.3213|\n+------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0.511,), (0.6232,), (0.4323,), (0.9434,), (0.3213,)],\n",
    "                           [\"values\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+--------+\n|values|features|\n+------+--------+\n| 0.511|     1.0|\n|0.6232|     1.0|\n|0.4323|     0.0|\n|0.9434|     1.0|\n|0.3213|     0.0|\n+------+--------+\n\n"
    }
   ],
   "source": [
    "binarizer = ft.Binarizer(threshold=0.5, inputCol=\"values\", outputCol='features')\n",
    "binarizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizer\n",
    "Similar to the Binarizer, this method takes a list of thresholds \n",
    "(the splits parameter) and transforms a continuous variable into a \n",
    "multinomial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+\n|values|\n+------+\n|   0.1|\n|   0.4|\n|   1.2|\n|   1.5|\n|   NaN|\n|   NaN|\n+------+\n\n"
    }
   ],
   "source": [
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, ['values'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+--------+\n|values|features|\n+------+--------+\n|   0.1|     0.0|\n|   0.4|     0.0|\n|   1.2|     1.0|\n|   1.5|     2.0|\n|   NaN|     3.0|\n|   NaN|     3.0|\n+------+--------+\n\n"
    }
   ],
   "source": [
    "bucketizer = ft.Bucketizer(\n",
    "    splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],\n",
    "    inputCol='values', outputCol='features'\n",
    ")\n",
    "# 保持 无效值NaN\n",
    "\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "bucket.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+---+\n|values|  b|\n+------+---+\n|   0.1|0.0|\n|   0.4|0.0|\n|   1.2|1.0|\n|   1.5|2.0|\n|   NaN|3.0|\n|   NaN|3.0|\n+------+---+\n\n"
    }
   ],
   "source": [
    "bucketizer.setParams(outputCol=\"b\").transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector\n",
    "使用卡方检验(Chi-Square) 完成特征选择\n",
    "\n",
    "$\\chi^2 -test$\n",
    "\n",
    "参考  https://blog.csdn.net/sinat_33761963/article/details/54910955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+-----+\n|          features|label|\n+------------------+-----+\n|[0.0,0.0,18.0,1.0]|  1.0|\n|[0.0,1.0,12.0,0.0]|  0.0|\n|[1.0,0.0,15.0,0.1]|  0.0|\n+------------------+-----+\n\n"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame(\n",
    "    [(Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0),\n",
    "     (Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.),\n",
    "     (Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.),],\n",
    "    [\"features\", 'label'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+-----+---------------+\n|          features|label|selectedFeature|\n+------------------+-----+---------------+\n|[0.0,0.0,18.0,1.0]|  1.0|         [18.0]|\n|[0.0,1.0,12.0,0.0]|  0.0|         [12.0]|\n|[1.0,0.0,15.0,0.1]|  0.0|         [15.0]|\n+------------------+-----+---------------+\n\n"
    }
   ],
   "source": [
    "# 选择最优的特征\n",
    "selector = ft.ChiSqSelector(numTopFeatures=1, outputCol='selectedFeature')\n",
    "model = selector.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2]"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "model.selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CountVectorizer\n",
    "处理标记文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+\n|label|            raw|\n+-----+---------------+\n|    0|      [a, b, c]|\n|    1|[a, b, b, c, a]|\n+-----+---------------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "    [\"label\", 'raw']\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+-------------------------+\n|label|raw            |vectors                  |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"
    }
   ],
   "source": [
    "cv = ft.CountVectorizer(minTF=1., minDF=1., \n",
    "                        inputCol='raw', outputCol='vectors')\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['a', 'b', 'c']"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.vocabulary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+---------------+-------------------------+\n|label|raw            |vectors                  |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"
    }
   ],
   "source": [
    "fromVocabModel = ft.CountVectorizerModel.from_vocabulary(\n",
    "    ['a', 'b', 'c'],\n",
    "    inputCol='raw',\n",
    "    outputCol='vectors')\n",
    "fromVocabModel.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCT\n",
    "A feature transformer that takes the 1D discrete cosine transform of a real vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElementwiseProduct\n",
    "\n",
    "元素级别的向量乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+\n|       values|\n+-------------+\n|[2.0,1.0,3.0]|\n+-------------+\n\n"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], \n",
    "                           [\"values\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+-------------+\n|       values|        eprod|\n+-------------+-------------+\n|[2.0,1.0,3.0]|[2.0,2.0,9.0]|\n+-------------+-------------+\n\n"
    }
   ],
   "source": [
    "ep = ft.ElementwiseProduct(\n",
    "    scalingVec=Vectors.dense([1.0, 2.0, 3.0]),\n",
    "    inputCol='values',\n",
    "    outputCol='eprod'\n",
    ")\n",
    "ep.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureHasher\n",
    "Feature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick (https://en.wikipedia.org/wiki/Feature_hashing) to map features to indices in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+-----+---------+------+\n|real| bool|stringNum|string|\n+----+-----+---------+------+\n| 2.0| true|        1|   foo|\n| 3.0|false|        2|   bar|\n+----+-----+---------+------+\n\n"
    }
   ],
   "source": [
    "data = [(2.0, True, \"1\", \"foo\"), (3.0, False, \"2\", \"bar\")]\n",
    "cols = [\"real\", \"bool\", \"stringNum\", \"string\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(262144, {174475: 2.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "hasher = ft.FeatureHasher(inputCols=cols, outputCol='features')\n",
    "hasher.transform(df).head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(262144, {171257: 1.0, 247670: 1.0, 257907: 1.0, 262126: 1.0})"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "hasher.setCategoricalCols([\"real\"]).transform(df).head().features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "\n",
    "输入为标记文本的列表, 返回一个带有计数的有预定长度的向量\n",
    "\n",
    "Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a `power of two` as the `numFeatures` parameter; otherwise the features will not be mapped evenly to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n",
    "hashingTF = ft.HashingTF(numFeatures=10, inputCol='words', outputCol='features')\n",
    "hashingTF.transform(df).head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SparseVector(16, {1: 1.0, 2: 1.0, 10: 1.0})"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "params = {hashingTF.numFeatures: 16, hashingTF.outputCol: \"vector\"}\n",
    "hashingTF.transform(df, params).head().vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}