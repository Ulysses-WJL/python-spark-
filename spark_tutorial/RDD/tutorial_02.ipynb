{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "base",
   "display_name": "tf-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SparkFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nConfiguration for a Spark application. Used to set various Spark\nparameters as key-value pairs.\n\nMost of the time, you would create a SparkConf object with\nC{SparkConf()}, which will load values from C{spark.*} Java system\nproperties as well. In this case, any parameters you set directly on\nthe C{SparkConf} object take priority over system properties.\n\nFor unit tests, you can also call C{SparkConf(false)} to skip\nloading external settings and get the same configuration no matter\nwhat the system properties are.\n\nAll setter methods in this class support chaining. For example,\nyou can write C{conf.setMaster(\"local\").setAppName(\"My app\")}.\n\n.. note:: Once a SparkConf object is passed to Spark, it is cloned\n    and can no longer be modified by the user.\n\u001b[0;31mInit docstring:\u001b[0m\nCreate a new Spark configuration.\n\n:param loadDefaults: whether to load values from Java system\n       properties (True by default)\n:param _jvm: internal parameter used to pass a handle to the\n       Java VM; does not need to be set by users\n:param _jconf: Optionally pass in an existing SparkConf handle\n       to use its parameters\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/conf.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "pyspark.SparkConf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set(key, value) − To set a configuration property.\n",
    "\n",
    "setMaster(value) − To set the master URL.\n",
    "\n",
    "setAppName(value) − To set an application name.\n",
    "\n",
    "get(key, defaultValue=None) − To get a configuration value of a key.\n",
    "\n",
    "setSparkHome(value) − To set Spark installation path on worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将SparkConf对象传递给Apache Spark\n",
    "conf = pyspark.SparkConf().setAppName('test').setMaster('local')  # spark://master:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"test_file\"\n",
    "sc.addFile(file_name)  # upload your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/tmp/spark-5cbd022d-96d8-40a8-9ce9-18b30287bee6/userFiles-6ba940ba-ae16-478a-951b-78978f5a3183/test_file'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "SparkFiles.get(\"test_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/tmp/spark-5cbd022d-96d8-40a8-9ce9-18b30287bee6/userFiles-6ba940ba-ae16-478a-951b-78978f5a3183'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializers\n",
    "\n",
    "Serialization is used for performance tuning on Apache Spark.  \n",
    "All data that is sent over the network or written to the disk or persisted in the memory should be serialized.\n",
    "\n",
    "- MarshalSerializer: Serializes objects using Python’s Marshal Serializer. This serializer is faster than PickleSerializer, but supports fewer datatypes\n",
    "\n",
    "- Serializes objects using Python’s Pickle Serializer. This serializer supports nearly any Python object, but may not be as fast as more specialized serializers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nSerializes objects using Python's pickle serializer:\n\n    http://docs.python.org/2/library/pickle.html\n\nThis serializer supports nearly any Python object, but may\nnot be as fast as more specialized serializers.\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/serializers.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     CloudPickleSerializer\n"
    }
   ],
   "source": [
    "pyspark.PickleSerializer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarshalSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nSerializes objects using Python's Marshal serializer:\n\n    http://docs.python.org/2/library/marshal.html\n\nThis serializer is faster than PickleSerializer but supports fewer datatypes.\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/serializers.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "pyspark.MarshalSerializer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "\n",
    "sc = SparkContext('local', 'serializer app', serializer=MarshalSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sc.parallelize(list(range(1000))).map(lambda x: x**2).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'local'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}