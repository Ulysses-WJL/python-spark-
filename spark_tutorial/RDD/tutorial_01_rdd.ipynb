{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "base",
   "display_name": "tf-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('RDD').setMaster('local[4]')\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'local[4]'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mDocstring:\u001b[0m\nreduce(function, sequence[, initial]) -> value\n\nApply a function of two arguments cumulatively to the items of a sequence,\nfrom left to right, so as to reduce the sequence to a single value.\nFor example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\nof the sequence in the calculation, and serves as a default when the\nsequence is empty.\n\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
    }
   ],
   "source": [
    "reduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext是任何spark功能的入口点.当我们运行任何Spark应用程序时，会启动一个驱动程序，它具有main函数，并且此处启动了SparkContext.然后，驱动程序在工作节点上的执行程序内运行操作.\n",
    "\n",
    "SparkContext使用Py4J启动 JVM 并创建 JavaSparkContext .默认情况下，PySpark将SparkContext作为'sc'提供，因此创建新的SparkContext将不起作用."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msparkHome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mjsc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'pyspark.profiler.BasicProfiler'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nMain entry point for Spark functionality. A SparkContext represents the\nconnection to a Spark cluster, and can be used to create L{RDD} and\nbroadcast variables on that cluster.\n\n.. note:: :class:`SparkContext` instance is not supported to share across multiple\n    processes out of the box, and PySpark does not guarantee multi-processing execution.\n    Use threads instead for concurrent processing purpose.\n\u001b[0;31mInit docstring:\u001b[0m\nCreate a new SparkContext. At least the master and app name should be set,\neither through the named parameters here or through C{conf}.\n\n:param master: Cluster URL to connect to\n       (e.g. mesos://host:port, spark://host:port, local[4]).\n:param appName: A name for your job, to display on the cluster web UI.\n:param sparkHome: Location where Spark is installed on cluster nodes.\n:param pyFiles: Collection of .zip or .py files to send to the cluster\n       and add to PYTHONPATH.  These can be paths on the local file\n       system or HDFS, HTTP, HTTPS, or FTP URLs.\n:param environment: A dictionary of environment variables to set on\n       worker nodes.\n:param batchSize: The number of Python objects represented as a single\n       Java object. Set 1 to disable batching, 0 to automatically choose\n       the batch size based on object sizes, or -1 to use an unlimited\n       batch size\n:param serializer: The serializer for RDDs.\n:param conf: A L{SparkConf} object setting Spark properties.\n:param gateway: Use an existing gateway and JVM, otherwise a new JVM\n       will be instantiated.\n:param jsc: The JavaSparkContext instance (optional).\n:param profiler_cls: A class of custom Profiler used to do profiling\n       (default is pyspark.profiler.BasicProfiler).\n\n\n>>> from pyspark.context import SparkContext\n>>> sc = SparkContext('local', 'test')\n\n>>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nValueError:...\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/context.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "SparkContext?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master : 它是它所连接的群集的URL.\n",
    "\n",
    "appName : 你的工作名称.\n",
    "\n",
    "sparkHome :  Spark安装目录.\n",
    "\n",
    "pyFiles : 要发送到集群并添加到PYTHONPATH的.zip或.py文件.\n",
    "\n",
    "环境 : 工作节点环境变量.\n",
    "\n",
    "batchSize : 表示为单个Java对象的Python对象数.设置1以禁用批处理，设置为0以根据对象大小自动选择批处理大小，或设置为-1以使用无限制的批处理大小.\n",
    "\n",
    "序列化程序 :  RDD序列化器.\n",
    "\n",
    "Conf :  L {SparkConf}的一个对象，用于设置所有Spark属性.\n",
    "\n",
    "网关 : 使用现有网关和JVM，否则初始化新JVM.\n",
    "\n",
    "JSC :  JavaSparkContext实例.\n",
    "\n",
    "profiler_cls : 用于分析的一类自定义Profiler(默认为pyspark.profiler.BasicProfiler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "log_file = \"file:///usr/local/spark/README.md\"\n",
    "log_data = sc.textFile(log_file).cache()\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['# Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data. It provides']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "log_data.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_A = log_data.filter(lambda s: 'a' in s).count()\n",
    "nums_B = log_data.filter(lambda s: 'b' in s).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(61, 30)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "nums_A, nums_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交任务\n",
    "```\n",
    "spark-submit first_app.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "RDD（Resilient Distributed Dataset）叫做`弹性分布式数据集`，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合\n",
    "\n",
    "\n",
    "1. 弹性\n",
    "- 存储的弹性：内存与磁盘的自动切换；\n",
    "- 容错的弹性：数据丢失可以自动恢复；\n",
    "- 计算的弹性：计算出错重试机制；\n",
    "- 分片的弹性：可根据需要重新分片。\n",
    "\n",
    "2. 分布式：数据存储在大数据集群不同节点上\n",
    "3. 数据集：RDD封装了计算逻辑，并不保存数据\n",
    "4. 数据抽象：RDD是一个抽象类，需要子类具体实现\n",
    "1. 不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑\n",
    "1. 可分区、并行计算\n",
    "\n",
    "要对这些RDD应用操作，有两种方法:\n",
    "\n",
    "- 转型(transform) : 这些是操作，它们应用于RDD以创建新的RDD. Filter，groupBy和map是转换的例子.\n",
    "\n",
    "- 动作(action) : 这些是应用于RDD的操作，它指示Spark执行计算并将结果发送回驱动程序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mjrdd_deserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nA Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\nRepresents an immutable, partitioned collection of elements that can be\noperated on in parallel.\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     PipelinedRDD\n"
    }
   ],
   "source": [
    "pyspark.RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 创建RDD单词，其中存储了一组提到的单词\n",
    "words = sc.parallelize(\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本 转换 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "counts = words.count()  # 返回RDD中的元素数\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['scala',\n 'java',\n 'hadoop',\n 'spark',\n 'akka',\n 'spark vs hadoop',\n 'pyspark',\n 'pyspark and spark']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "coll = words.collect()  # 返回RDD中的所有元素\n",
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nApplies a function to all elements of this RDD.\n\n>>> def f(x): print(x)\n>>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "words.foreach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(f) 仅返回满足foreach内函数条件的元素\n",
    "def f(x): print(x)\n",
    "\n",
    "words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# filter(f) 返回一个包含元素的新RDD，它满足过滤器内部的功能\n",
    "words.filter(lambda x: 'spark' in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['scala_001',\n 'java_001',\n 'hadoop_001',\n 'spark_001',\n 'akka_001',\n 'spark vs hadoop_001',\n 'pyspark_001',\n 'pyspark and spark_001']"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# map(f，preservesPartitioning = False)\n",
    "# 通过将函数应用于RDD中的每个元素来返回一个新的RDD\n",
    "words.map(lambda x: x + '_001').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[False, False, False, True, False, True, True, True]"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "words.map(lambda x: 'spark' in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# reduce(f)\n",
    "# 执行指定的可交换和关联二进制运算后，返回RDD中的元素\n",
    "nums = sc.parallelize(range(10))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "45"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "from operator import add\n",
    "nums.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2, 3, 4, 5]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# distinct 运算\n",
    "nums = sc.parallelize([1, 2, 3, 3, 4, 4, 5, 5, 5])\n",
    "nums.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = nums.randomSplit([0.4, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 4, 5, 5]"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "num_list[0].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2, 3, 3, 4, 5]"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "num_list[1].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('even', <pyspark.resultiterable.ResultIterable at 0x7fb09a2517d0>),\n ('odd', <pyspark.resultiterable.ResultIterable at 0x7fb09a251810>)]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# groupBy \n",
    "nums = sc.parallelize(range(10))\n",
    "g_RDD = nums.groupBy(lambda x: 'even' if (x % 2 == 0) else \"odd\").collect()\n",
    "g_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "even [0, 2, 4, 6, 8]\n"
    }
   ],
   "source": [
    "print(g_RDD[0][0], sorted(g_RDD[0][1]))  # 偶数list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "odd [1, 3, 5, 7, 9]\n"
    }
   ],
   "source": [
    "print(g_RDD[1][0], sorted(g_RDD[1][1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('hadoop', (4, 5)), ('spark', (1, 2))]"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# join(other, numPartitions = None)\n",
    "# It returns RDD with a pair of elements with the matching keys and all the values for that particular key. \n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ParallelCollectionRDD[11] at parallelize at PythonRDD.scala:195"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# cache() 持久化\n",
    "# Persist this RDD with the default storage level (MEMORY_ONLY)\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "words.persist().is_cached "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个RDD 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([2, 4])\n",
    "rdd3 = sc.parallelize([4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2, 2, 4, 4, 8]"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# 并集\n",
    "rdd1.union(rdd2).union(rdd3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2]"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# 交集\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1]"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# 差集\n",
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 2), (1, 4), (2, 2), (2, 4)]"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# 笛卡尔积\n",
    "rdd1.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本 动作 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([2, 1, 0, 4, 3, 5, 7, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "nums.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2, 1, 0]"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, 1, 2]"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "nums.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[8, 7, 6]"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "nums.takeOrdered(3, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(count: 9, mean: 4.0, stdev: 2.5819888974716116, max: 8.0, min: 0.0)"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# 一些统计功能\n",
    "nums.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0, 8, 2.5819888974716116, 4.0, 9, 36)"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "nums.min(), nums.max(), nums.stdev(), nums.mean(), nums.count(), nums.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD key-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvrdd = sc.parallelize([('a', 1), ('b', 2), ('c', 3), ('d', 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['a', 'b', 'c', 'd']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "kvrdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2, 3, 4]"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "kvrdd.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', 1), ('b', 2), ('c', 3), ('d', 4)]"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "kvrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('c', 3), ('d', 4)]"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "kvrdd.filter(lambda x: x[0]>'b').collect()  # 针对key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('c', 3), ('d', 4)]"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "kvrdd.filter(lambda x: x[1]>2).collect()  # 针对value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', 1), ('b', 4), ('c', 9), ('d', 16)]"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "kvrdd.mapValues(lambda x: x**2).collect()   # 对组的value进行操作kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('d', 4), ('c', 3), ('b', 2), ('a', 1)]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "kvrdd.sortByKey(ascending=False).collect()  # 按key 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvrdd2 = sc.parallelize([('a', 1), ('a', 2), ('b', 3), ('b', 4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('b', 7), ('a', 3)]"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "kvrdd2.reduceByKey(lambda x1, x2: x1 + x2).collect()  # 相同的key进行reduce运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('b', 7), ('a', 3)]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# groupByKey 结果 (b, iterable的result对象), (a, ), ...\n",
    "kvrdd2.groupByKey().map(lambda x: (x[0], sum(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('b', (3, '3')),\n ('b', (3, '4')),\n ('b', (4, '3')),\n ('b', (4, '4')),\n ('a', (1, '1')),\n ('a', (1, '2')),\n ('a', (2, '1')),\n ('a', (2, '2'))]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "kvrdd3 = sc.parallelize([('a', '1'), ('a', '2'), ('b', '3'), ('b', '4')])\n",
    "kvrdd2.join(kvrdd3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "chr(ord('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', 0), ('b', 1), ('c', 2), ('d', 3), ('e', 4), ('f', 5)]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "kvrdd4 = sc.parallelize((chr(ord('a') + i), i) for i in range(6))\n",
    "kvrdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', '0'), ('c', '2'), ('e', '4'), ('g', '6'), ('i', '8')]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "kvrdd5 = sc.parallelize((chr(ord('a') + i), '{}'.format(i)) for i in range(10) if i % 2 ==0)\n",
    "kvrdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('c', (2, '2')), ('a', (0, '0')), ('e', (4, '4'))]"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# join  inner\n",
    "kvrdd4.join(kvrdd5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', (0, '0')),\n ('b', (1, None)),\n ('c', (2, '2')),\n ('d', (3, None)),\n ('e', (4, '4')),\n ('f', (5, None))]"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# left join\n",
    "kvrdd4.leftOuterJoin(kvrdd5).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', (0, '0')),\n ('c', (2, '2')),\n ('e', (4, '4')),\n ('g', (None, '6')),\n ('i', (None, '8'))]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# right join\n",
    "kvrdd4.rightOuterJoin(kvrdd5).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', (0, '0')),\n ('b', (1, None)),\n ('c', (2, '2')),\n ('d', (3, None)),\n ('e', (4, '4')),\n ('f', (5, None)),\n ('g', (None, '6')),\n ('i', (None, '8'))]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# full out join \n",
    "kvrdd4.fullOuterJoin(kvrdd5).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('b', 1), ('d', 3), ('f', 5)]"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# 删除 相同key的数据\n",
    "kvrdd4.subtractByKey(kvrdd5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('a', 0)"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "kvfirst= kvrdd4.first()\n",
    "kvfirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'a'"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "kvfirst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "kvfirst[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('a', 0), ('b', 1)]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "kvrdd4.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(int, {'a': 2, 'b': 2})"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "kvrdd2.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5}"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "kvrdd4.collectAsMap()  # 创建python 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0]"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "kvrdd4.lookup('a')  # 输入key值 寻找value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2]"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "kvrdd2.lookup('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " key表示图书名称，value表示某天图书销量，请计算每个键对应的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('hadoop', 5.0), ('spark', 4.0)]"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# key表示图书名称，value表示某天图书销量，请计算每个键对应的平均值\n",
    "rdd = sc.parallelize([(\"spark\",2),(\"hadoop\",6),(\"hadoop\",4),(\"spark\",6)])\n",
    "# (k, (销量, 1))) -> (k, (销量和, 数量和)))\n",
    "rdd.mapValues(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "    .mapValues(lambda x: x[0] / x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast & Accumulator\n",
    "\n",
    "For parallel processing, Apache Spark uses shared variables. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvFruit = sc.parallelize([(1, \"apple\"), (2, 'orange'), (3, 'banana'), (4, 'grape')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 'apple', 2: 'orange', 3: 'banana', 4: 'grape'}"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "fruit_map = kvFruit.collectAsMap()\n",
    "fruit_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "水果编号 [2, 4, 3, 1]\n"
    }
   ],
   "source": [
    "fruit_ids = sc.parallelize([2, 4, 3, 1])\n",
    "print(\"水果编号\", fruit_ids.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['orange', 'grape', 'banana', 'apple']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# 使用字典进行转换\n",
    "fruit_names = fruit_ids.map(lambda x: fruit_map[x]).collect()\n",
    "fruit_names  # 每执行一次转换都需要将二者传送到Worker node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables are used to save the copy of data across all nodes. \n",
    "This variable is cached on all the machines and not sent on machines with tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.broadcast.Broadcast at 0x7f7a3da19d50>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "bc_fruit = sc.broadcast(fruit_map)\n",
    "bc_fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['orange', 'grape', 'banana', 'apple']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# # Broadcast变量有一个名为value的属性，它存储数据并用于返回广播值\n",
    "fruit_names = fruit_ids.map(lambda x: bc_fruit.value[x]).collect()\n",
    "fruit_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bc_fruit 会传送到Worker Node 机器, 并存储在内存中.\n",
    "\n",
    "广播变量被创建后不能修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulator variables are used for aggregating the information through associative and commutative operations. \n",
    "\n",
    "For example, you can use an accumulator for a sum operation or counters (in MapReduce). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Accumulator<id=0, value=10>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "num = sc.accumulator(10)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    global num\n",
    "    num += x\n",
    "rdd = sc.parallelize([10, 20, 30, 40])\n",
    "rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "110"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "num.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用add进行累加\n",
    "\n",
    "num_float = sc.accumulator(0.0)  # double 类型\n",
    "num_int = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: [num_float.add(x), num_int.add(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(100.0, 100)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "num_float.value, num_int.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(float, int)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "type(num_float.value), type(num_int.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 持久化\n",
    "标记为持久化, 要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化\n",
    "\n",
    "将需要重复运算的RDD 存储在内存中, 提升计算效率\n",
    "- RDD.presist(存储等级) -默认MEMORY_ONLY\n",
    "- RDD.unpresist()\n",
    "\n",
    "This can only be used to assign a new storage level if the RDD does not have a storage level set yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorageLevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nSet this RDD's storage level to persist its values across operations\nafter the first time it is computed. This can only be used to assign\na new storage level if the RDD does not have a storage level set yet.\nIf no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n>>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n>>> rdd.persist().is_cached\nTrue\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "rdd.persist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0museDisk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0museMemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0museOffHeap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdeserialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreplication\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nFlags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\nwhether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\nin a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\nnodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\nSince the data is always serialized on the Python side, all the constants use the serialized\nformats.\n\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/storagelevel.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "pyspark.StorageLevel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:195"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "rdd_memory = sc.parallelize([3, 1, 3, 4])\n",
    "rdd_memory.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "rdd_memory.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "rdd_memory.unpersist()\n",
    "rdd_memory.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "rdd_memory_disk  = sc.parallelize([1, 2, 3])\n",
    "rdd_memory_disk.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "rdd_memory_disk.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StorageLevel(True, True, False, False, 1)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "rdd_memory_disk.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "file:///usr/local/spark/README.md MapPartitionsRDD[10] at textFile at NativeMethodAccessorImpl.java:0"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "log_file = sc.textFile(\"file:///usr/local/spark/README.md\")\n",
    "log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = log_file.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('', 72),\n ('the', 23),\n ('to', 17),\n ('Spark', 15),\n ('for', 12),\n ('and', 10),\n ('a', 9),\n ('##', 9),\n ('is', 7),\n ('on', 7)]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# result = counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "result = counts_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_rdd.saveAsTextFile(\"data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('#', 1)\n('Apache', 1)\n('Spark', 15)\n('', 72)\n('is', 7)\n('a', 9)\n('fast', 1)\n('and', 10)\n('general', 3)\n('cluster', 2)\n('computing', 1)\n('system', 1)\n('for', 12)\n('Big', 1)\n('Data.', 1)\n('It', 2)\n('provides', 1)\n('high-level', 1)\n('APIs', 1)\n('in', 5)\n('Scala,', 1)\n('Java,', 1)\n('Python,', 2)\n('R,', 1)\n('an', 4)\n('optimized', 1)\n('engine', 1)\n('that', 2)\n('supports', 2)\n('computation', 1)\n('graphs', 1)\n('data', 1)\n('analysis.', 1)\n('also', 5)\n('rich', 1)\n('set', 2)\n('of', 5)\n('higher-level', 1)\n('tools', 1)\n('including', 4)\n('SQL', 2)\n('DataFrames,', 1)\n('MLlib', 1)\n('machine', 1)\n('learning,', 1)\n('GraphX', 1)\n('graph', 1)\n('processing,', 1)\n('Streaming', 1)\n('stream', 1)\n('processing.', 1)\n('<http://spark.apache.org/>', 1)\n('##', 9)\n('Online', 1)\n('Documentation', 1)\n('You', 3)\n('can', 6)\n('find', 1)\n('the', 23)\n('latest', 1)\n('documentation,', 1)\n('programming', 1)\n('guide,', 1)\n('on', 7)\n('[project', 1)\n('web', 1)\n('page](http://spark.apache.org/documentation.html).', 1)\n('This', 2)\n('README', 1)\n('file', 1)\n('only', 1)\n('contains', 1)\n('basic', 1)\n('setup', 1)\n('instructions.', 1)\n('Building', 1)\n('built', 1)\n('using', 3)\n('[Apache', 1)\n('Maven](http://maven.apache.org/).', 1)\n('To', 2)\n('build', 3)\n('its', 1)\n('example', 3)\n('programs,', 1)\n('run:', 1)\n('build/mvn', 1)\n('-DskipTests', 1)\n('clean', 1)\n('package', 1)\n('(You', 1)\n('do', 2)\n('not', 1)\n('need', 1)\n('to', 17)\n('this', 1)\n('if', 4)\n('you', 4)\n('downloaded', 1)\n('pre-built', 1)\n('package.)', 1)\n('More', 1)\n('detailed', 2)\n('documentation', 3)\n('available', 1)\n('from', 1)\n('project', 1)\n('site,', 1)\n('at', 2)\n('[\"Building', 1)\n('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)\n('For', 3)\n('development', 1)\n('tips,', 1)\n('info', 1)\n('developing', 1)\n('IDE,', 1)\n('see', 3)\n('[\"Useful', 1)\n('Developer', 1)\n('Tools\"](http://spark.apache.org/developer-tools.html).', 1)\n('Interactive', 2)\n('Scala', 2)\n('Shell', 2)\n('The', 1)\n('easiest', 1)\n('way', 1)\n('start', 1)\n('through', 1)\n('shell:', 2)\n('./bin/spark-shell', 1)\n('Try', 1)\n('following', 2)\n('command,', 2)\n('which', 2)\n('should', 2)\n('return', 2)\n('1000:', 2)\n('scala>', 1)\n('sc.parallelize(1', 1)\n('1000).count()', 1)\n('Python', 2)\n('Alternatively,', 1)\n('prefer', 1)\n('use', 3)\n('./bin/pyspark', 1)\n('And', 1)\n('run', 7)\n('>>>', 1)\n('sc.parallelize(range(1000)).count()', 1)\n('Example', 1)\n('Programs', 1)\n('comes', 1)\n('with', 3)\n('several', 1)\n('sample', 1)\n('programs', 2)\n('`examples`', 2)\n('directory.', 1)\n('one', 2)\n('them,', 1)\n('`./bin/run-example', 1)\n('<class>', 1)\n('[params]`.', 1)\n('example:', 1)\n('./bin/run-example', 2)\n('SparkPi', 2)\n('will', 1)\n('Pi', 1)\n('locally.', 1)\n('MASTER', 1)\n('environment', 1)\n('variable', 1)\n('when', 1)\n('running', 1)\n('examples', 2)\n('submit', 1)\n('cluster.', 1)\n('be', 2)\n('mesos://', 1)\n('or', 3)\n('spark://', 1)\n('URL,', 1)\n('\"yarn\"', 1)\n('YARN,', 1)\n('\"local\"', 1)\n('locally', 2)\n('thread,', 1)\n('\"local[N]\"', 1)\n('N', 1)\n('threads.', 1)\n('abbreviated', 1)\n('class', 2)\n('name', 1)\n('package.', 1)\n('instance:', 1)\n('MASTER=spark://host:7077', 1)\n('Many', 1)\n('print', 1)\n('usage', 1)\n('help', 1)\n('no', 1)\n('params', 1)\n('are', 1)\n('given.', 1)\n('Running', 1)\n('Tests', 1)\n('Testing', 1)\n('first', 1)\n('requires', 1)\n('[building', 1)\n('Spark](#building-spark).', 1)\n('Once', 1)\n('built,', 1)\n('tests', 2)\n('using:', 1)\n('./dev/run-tests', 1)\n('Please', 4)\n('guidance', 2)\n('how', 3)\n('[run', 1)\n('module,', 1)\n('individual', 1)\n('tests](http://spark.apache.org/developer-tools.html#individual-tests).', 1)\n('There', 1)\n('Kubernetes', 1)\n('integration', 1)\n('test,', 1)\n('resource-managers/kubernetes/integration-tests/README.md', 1)\n('A', 1)\n('Note', 1)\n('About', 1)\n('Hadoop', 3)\n('Versions', 1)\n('uses', 1)\n('core', 1)\n('library', 1)\n('talk', 1)\n('HDFS', 1)\n('other', 1)\n('Hadoop-supported', 1)\n('storage', 1)\n('systems.', 1)\n('Because', 1)\n('protocols', 1)\n('have', 1)\n('changed', 1)\n('different', 1)\n('versions', 1)\n('Hadoop,', 2)\n('must', 1)\n('against', 1)\n('same', 1)\n('version', 1)\n('your', 1)\n('runs.', 1)\n('refer', 2)\n('[\"Specifying', 1)\n('Version', 1)\n('Enabling', 1)\n('YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 1)\n('building', 2)\n('particular', 2)\n('distribution', 1)\n('Hive', 2)\n('Thriftserver', 1)\n('distributions.', 1)\n('Configuration', 1)\n('[Configuration', 1)\n('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1)\n('online', 1)\n('overview', 1)\n('configure', 1)\n('Spark.', 1)\n('Contributing', 1)\n('review', 1)\n('[Contribution', 1)\n('guide](http://spark.apache.org/contributing.html)', 1)\n('information', 1)\n('get', 1)\n('started', 1)\n('contributing', 1)\n('project.', 1)\n"
    }
   ],
   "source": [
    "%cat data/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nReturn a new RDD by first applying a function to all elements of this\nRDD, and then flattening the results.\n\n>>> rdd = sc.parallelize([2, 3, 4])\n>>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n[1, 1, 1, 2, 2, 3]\n>>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "log_file.flatMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nReturn a new RDD by applying a function to each element of this RDD.\n\n>>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n>>> sorted(rdd.map(lambda x: (x, 1)).collect())\n[('a', 1), ('b', 1), ('c', 1)]\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "log_file.map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap 与 map的差异**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = sc.textFile(\"test_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mnt/data1/workspace/data_analysis_mining/spark/tutorial/test_file\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cbb5850974aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 具有分层结构\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mnt/data1/workspace/data_analysis_mining/spark/tutorial/test_file\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "test_file.map(lambda line: line.split(\" \")).collect()  # 具有分层结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['apple',\n 'orange',\n 'apple',\n 'maple',\n 'banana',\n 'orange',\n 'water',\n 'mountain',\n '',\n 'first',\n 'apple']"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "str_rdd = test_file.flatMap(lambda line: line.split(\" \"))  # flattening the results\n",
    "str_rdd.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分区\n",
    "\n",
    "RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上\n",
    "\n",
    "1. 分区的作用\n",
    "\n",
    "- 增加并行度\n",
    "- 减少通信开销\n",
    "\n",
    "\n",
    "2. RDD分区原则\n",
    "\n",
    "RDD分区的一个原则是使得分区的个数尽量等于集群中的CPU核心（core）数目\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nDistribute a local Python collection to form an RDD. Using xrange\nis recommended if the input represents a range for performance.\n\n>>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n[[0], [2], [3], [4], [6]]\n>>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n[[], [0], [], [2], [4]]\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/context.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nRead a text file from HDFS, a local file system (available on all\nnodes), or any Hadoop-supported file system URI, and return it as an\nRDD of Strings.\n\nIf use_unicode is False, the strings will be kept as `str` (encoding\nas `utf-8`), which is faster and smaller than unicode. (Added in\nSpark 1.2)\n\n>>> path = os.path.join(tempdir, \"sample-text.txt\")\n>>> with open(path, \"w\") as testFile:\n...    _ = testFile.write(\"Hello world!\")\n>>> textFile = sc.textFile(path)\n>>> textFile.collect()\n['Hello world!']\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/context.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "sc.textFile?  # minPartition 手动指定分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"file:///usr/local/spark/README.md\"\n",
    "log_data = sc.textFile(log_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = sc.parallelize(range(10), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nReturn an RDD created by coalescing all elements within each partition\ninto a list.\n\n>>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n>>> sorted(rdd.glom().collect())\n[[1, 2], [3, 4]]\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "l.glom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "l.glom().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 重新设置分区\n",
    "new_l = l.repartition(1)\n",
    "new_l.glom().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自定义分区\n",
    "\n",
    "Spark提供了自带的HashPartitioner（哈希分区）与RangePartitioner（区域分区），能够满足大多数应用场景的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "def MyPartitioner(key):\n",
    "    print(\"MyPartitioner is running\")\n",
    "    print('The key is %d' % key)\n",
    "    return key%10\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"The main function is running\")\n",
    "    conf = SparkConf().setMaster(\"local\").setAppName(\"MyApp\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "    data=sc.parallelize(range(10),5)\n",
    "    data.map(lambda x:(x,1)) \\\n",
    "        .partitionBy(10,MyPartitioner) \\\n",
    "        .map(lambda x:x[0]) \\\n",
    "        .saveAsTextFile(\"file:///usr/local/spark/mycode/rdd/partitioner\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyPartitioner(key):\n",
    "    print(\"MyPartitioner is running\")\n",
    "    print('The key is %d' % key)\n",
    "    return key%10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mportable_hash\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f7f22f9e3b0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nReturn a copy of the RDD partitioned using the specified partitioner.\n\n>>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n>>> sets = pairs.partitionBy(2).glom().collect()\n>>> len(set(sets[0]).intersection(set(sets[1])))\n0\n\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/rdd.py\n\u001b[0;31mType:\u001b[0m      method\n"
    }
   ],
   "source": [
    "data=sc.parallelize(range(10),5)\n",
    "#  # 只接受key-value形式\n",
    "data.partitionBy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.map(lambda x: (x, 1))\\\n",
    "    .partitionBy(10, MyPartitioner)\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .saveAsTextFile('partition_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子: 二次排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5 3\n1 6\n4 9\n8 3\n4 7\n5 6\n3 2\n"
    }
   ],
   "source": [
    "%cat data/file4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_04 = sc.textFile(\"file:///mnt/data1/workspace/data_analysis_mining/Python+Spark2.0+Hadoop机器学习与大数据实战/spark_tutorial/data/file4.txt\")\n",
    "rdd_04.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[((5, 3), '5 3'),\n ((1, 6), '1 6'),\n ((4, 9), '4 9'),\n ((8, 3), '8 3'),\n ((4, 7), '4 7'),\n ((5, 6), '5 6'),\n ((3, 2), '3 2')]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "res1 = rdd_04.filter(lambda line: len(line.strip()) > 0)\n",
    "\n",
    "res2 = res1.map(\n",
    "    lambda x: ((int(x.split(\" \")[0]), int(x.split(\" \")[1])),\n",
    "               x)\n",
    ")\n",
    "res2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import gt\n",
    "\n",
    "class SecondarySort:\n",
    "    def __init__(self, k):\n",
    "        self.c1 = k[0]\n",
    "        self.c2 = k[1]\n",
    "    def __gt__(self, other):\n",
    "        if other.c1 == self.c1:\n",
    "            return gt(self.c2, other.c2)\n",
    "        else:\n",
    "            return gt(self.c1, other.c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 30, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 404, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.SecondarySort'>: attribute lookup SecondarySort on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 404, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.SecondarySort'>: attribute lookup SecondarySort on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cd9e942c58bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSecondarySort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mres5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mmaxSampleSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20.0\u001b[0m  \u001b[0;31m# constant from Spark's RangePartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxSampleSize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrddSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 30, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 404, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.SecondarySort'>: attribute lookup SecondarySort on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 404, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.SecondarySort'>: attribute lookup SecondarySort on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "res3 = res2.map(lambda x: (SecondarySort(x[0]), x[1]))\n",
    "res4 = res3.sortByKey(False)\n",
    "res5 = res4.map(lambda x: x[1])\n",
    "res5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}